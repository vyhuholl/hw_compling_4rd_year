{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9ffEpiC0J3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OIA9_wV0OkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install pycodestyle flake8 pycodestyle_magic\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1x1J8X70Uuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext pycodestyle_magic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeHT0N80zc3T",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "Using text http://www.gutenberg.org/files/2600/2600-0.txt\n",
        "1. Make text lowercase and remove all punctuation except spaces and dots.\n",
        "2. Tokenize text by BPE with vocab_size = 100\n",
        "3. Train 3-gram language model with laplace smoothing $\\delta=1$\n",
        "4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens.\n",
        "5. Calculate perplexity of the language model for the first sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTkIT1a1z2G1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://www.gutenberg.org/files/2600/2600-0.txt -O peace.txt\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwc3hNhmzc3Y",
        "colab_type": "code",
        "outputId": "17840b2f-edd3-4e4c-982a-e191b1376e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = open('peace.txt', 'r').read()[2:]\n",
        "len(text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3227579"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvQe0O6_1I8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY1pkpaKzc3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regex = r\"([\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\/\\:\\;\\<\\=\\>\\?\\@\\[\\]\\^\\_\\`\\{\\|\\}\\~\\“\\”\\‘\\’\\—]|\\s)+\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    return re.sub(regex, \" \", text.lower())\n",
        "\n",
        "text = preprocess_text(text)\n",
        "assert len(text) == 3141169"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22S73H3Pzc3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = text.split('.')\n",
        "text = [x.strip() for x in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0cLL7yywowd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "from sklearn.base import TransformerMixin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EQGQ8hJgzc31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BPE(TransformerMixin):\n",
        "    def __init__(self, vocab_size=100):\n",
        "        super(BPE, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # index to token\n",
        "        self.itos = []\n",
        "        # token to index\n",
        "        self.stoi = {}\n",
        "\n",
        "    def fit(self, text):\n",
        "        \"\"\"\n",
        "        fit itos and stoi\n",
        "        text: list of strings\n",
        "        \"\"\"\n",
        "        self.itos = list(set(\"\".join(text)))\n",
        "        self.stoi = {token: index for index, token in enumerate(self.itos)}\n",
        "        text = [[self.stoi[char] for char in s] for s in text]\n",
        "\n",
        "        while len(self.itos) < self.vocab_size:\n",
        "            new_token = Counter([(s[i], s[i + 1])\n",
        "                                 for s in text\n",
        "                                 for i in range(len(s) - 1)]\n",
        "                                ).most_common(1)[0][0]\n",
        "            # most common bigram in the text\n",
        "            new_id = len(self.itos)\n",
        "\n",
        "            self.itos.append(new_token)\n",
        "            self.stoi[new_token] = new_id\n",
        "\n",
        "            # find occurences of the new_token in the text\n",
        "            # and replace them with new_id\n",
        "            for i, s in enumerate(text):\n",
        "                j = 0\n",
        "                while j < len(s) - 1:\n",
        "                    if (s[j], s[j + 1]) == new_token:\n",
        "                        text[i][j] = new_id\n",
        "                        text[i].pop(j + 1)\n",
        "                    j += 1\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, text):\n",
        "        \"\"\"\n",
        "        convert text to a sequence of token ids\n",
        "        text: list of strings\n",
        "        \"\"\"\n",
        "        text = [[self.stoi[char] for char in s] for s in text]\n",
        "        for token_id, token in enumerate(self.itos):\n",
        "            # find occurences of the token in the text\n",
        "            # and replace them with token_id\n",
        "            if type(token) == tuple:\n",
        "                for i, s in enumerate(text):\n",
        "                    j = 0\n",
        "                    while j < len(s) - 1:\n",
        "                        if (s[j], s[j + 1]) == token:\n",
        "                            text[i][j] = token_id\n",
        "                            text[i].pop(j + 1)\n",
        "                        j += 1\n",
        "        return text\n",
        "\n",
        "    def decode_token(self, tok):\n",
        "        \"\"\"\n",
        "        tok: int or tuple\n",
        "        \"\"\"\n",
        "        result = \"\" if tok > len(self.itos) else self.itos[tok]\n",
        "        if type(result) == tuple:\n",
        "            return self.decode_token(result[0]) + self.decode_token(result[1])\n",
        "        return result\n",
        "\n",
        "    def decode(self, text):\n",
        "        \"\"\"\n",
        "        convert token ids into text\n",
        "        \"\"\"\n",
        "        return ''.join(map(self.decode_token, text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR6MmRBRwsTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 100\n",
        "bpe = BPE(vocab_size)\n",
        "tokenized_text = bpe.fit_transform(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVjMF00Szc36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert bpe.decode(tokenized_text[0]) == text[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXY1pkhGzc4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "start_token = vocab_size\n",
        "end_token = vocab_size + 1\n",
        "\n",
        "\n",
        "class LM:\n",
        "    def __init__(self, vocab_size, delta=1):\n",
        "        self.delta = delta\n",
        "        self.vocab_size = vocab_size + 2\n",
        "        self.proba = np.full((self.vocab_size,\n",
        "                              self.vocab_size,\n",
        "                              self.vocab_size),\n",
        "                             delta)\n",
        "\n",
        "    def infer(self, a, b, tau=1):\n",
        "        \"\"\"\n",
        "        return vector of probabilities of size self.vocab\n",
        "        for 3-grams which start with (a,b) tokens\n",
        "        a: first token id\n",
        "        b: second token id\n",
        "        tau: temperature\n",
        "        \"\"\"\n",
        "        result = np.array([self.get_proba(a, b, i, tau)\n",
        "                           for i in range(self.vocab_size)])\n",
        "        return result\n",
        "\n",
        "    def get_proba(self, a, b, c, tau=1):\n",
        "        \"\"\"\n",
        "        get probability of 3-gram (a,b,c)\n",
        "        a: first token id\n",
        "        b: second token id\n",
        "        c: third token id\n",
        "        tau: temperature\n",
        "        \"\"\"\n",
        "        result = (self.proba[a][b][c] ** (1 / tau)) / \\\n",
        "                 ((self.proba[a][b] ** (1 / tau)).sum())\n",
        "        return result\n",
        "\n",
        "    def fit(self, text):\n",
        "        \"\"\"\n",
        "        train language model on text\n",
        "        text: list of lists\n",
        "        \"\"\"\n",
        "        for s in text:\n",
        "            s = [start_token] + s + [end_token]\n",
        "            for i in range(len(s) - 2):\n",
        "                self.proba[s[i]][s[i + 1]][s[i + 2]] += 1\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "lm = LM(vocab_size, 1).fit(tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grIpPiAKzc4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
        "    \"\"\"\n",
        "    generate sequence from language model *lm* conditioned on input_seq\n",
        "    input_seq: sequence of token ids for conditioning\n",
        "    lm: language model\n",
        "    max_len: max generated sequence length\n",
        "    k: size of beam\n",
        "    tau: temperature\n",
        "    \"\"\"\n",
        "    beam = [((input_seq[-2], input_seq[-1], token),\n",
        "             lm.get_proba(input_seq[-2], input_seq[-1], token, tau=tau))\n",
        "            for token in np.argsort(lm.infer(input_seq[-2],\n",
        "                                             input_seq[-1],\n",
        "                                             tau=tau))[::-1][:k]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        candidates = []\n",
        "        candidates_proba = []\n",
        "        for snt, snt_proba in beam:\n",
        "            if snt[-1] == end_token:\n",
        "                candidates.append(snt)\n",
        "                candidates_proba.append(snt_proba)\n",
        "            else:\n",
        "                proba = lm.infer(snt[-2], snt[-1], tau)\n",
        "                best_k = np.argsort(proba)[::-1][:k]\n",
        "                for candidate in best_k:\n",
        "                    candidates.append(snt + (candidate,))\n",
        "                    candidates_proba.append(snt_proba * lm.get_proba(snt[-2],\n",
        "                                                                     snt[-1],\n",
        "                                                                     candidate,\n",
        "                                                                     tau=tau))\n",
        "        beam = [(candidates[i], candidates_proba[i])\n",
        "                for i in np.argsort(candidates_proba)[::-1][:k]]\n",
        "    return beam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECPsT2_2zc4E",
        "colab_type": "code",
        "outputId": "25e30369-d120-4b38-9e7f-25f4d8aa0402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "input1 = 'horse '\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "for s, proba in result:\n",
        "    print(f\"{bpe.decode(input1)}{bpe.decode(s[2:])};\\tlog proba: {proba}.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "horse with a smill ;\tlog proba: 0.6841029778083704.\n",
            "horse was not been sa;\tlog proba: 0.0417781077348771.\n",
            "horse was sold not be;\tlog proba: 0.02009117223303252.\n",
            "horse when said not ;\tlog proba: 0.019575605374780952.\n",
            "horse the countess mary ;\tlog proba: 0.012419496299872266.\n",
            "horse who had been sa;\tlog proba: 0.01086774490413632.\n",
            "horse with his fack ;\tlog proba: 0.007780060616618096.\n",
            "horse the counderstand wi;\tlog proba: 0.007335131282565679.\n",
            "horse the cound him and s;\tlog proba: 0.007187195139002838.\n",
            "horse the cound him and w;\tlog proba: 0.0057511169654040715.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KbzJHPMzc4G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "aba5645e-4536-4c3a-8462-de6e77919276"
      },
      "source": [
        "input1 = 'her'\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "for s, proba in result:\n",
        "    print(f\"{bpe.decode(input1)}{bpe.decode(s[2:])};\\tlog proba: {proba}.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here with a smill;\tlog proba: 0.7304666014228688.\n",
            "here was not been s;\tlog proba: 0.04874047901633911.\n",
            "here was sold not b;\tlog proba: 0.02279761621687215.\n",
            "here when said no;\tlog proba: 0.020902320798071278.\n",
            "here who had been s;\tlog proba: 0.012678867502004655.\n",
            "here was she was not b;\tlog proba: 0.009807273664700423.\n",
            "here with his heas;\tlog proba: 0.009110050541107705.\n",
            "here with his fack;\tlog proba: 0.008307338307196785.\n",
            "here with his hear ;\tlog proba: 0.007996051526025703.\n",
            "here was not seemp;\tlog proba: 0.006440570045187267.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3XSoQ2Fzc4J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "94d26c02-1d92-41f0-9f41-ab816f27db8a"
      },
      "source": [
        "input1 = 'what'\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=1)\n",
        "for s, proba in result:\n",
        "    print(f\"{bpe.decode(input1)}{bpe.decode(s[2:])};\\tlog proba: {proba}.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what;\tlog proba: 0.02443609022556391.\n",
            "whated;\tlog proba: 0.011341031957494033.\n",
            "whated to him;\tlog proba: 0.0002072659731327774.\n",
            "what theight;\tlog proba: 5.717776867622187e-05.\n",
            "whated to himself;\tlog proba: 1.6978380760763765e-05.\n",
            "whated to himself it ;\tlog proba: 1.702154199055581e-06.\n",
            "whated to himself i ;\tlog proba: 1.4922512460114212e-06.\n",
            "whated to himself sa;\tlog proba: 9.303074810315937e-07.\n",
            "whated to himself he ha;\tlog proba: 8.943145534674918e-07.\n",
            "whated to himself he w;\tlog proba: 8.367494787615383e-07.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zOZXaxizc4L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "877fe0d5-3c74-4b3b-bd3d-123f3fbe0276"
      },
      "source": [
        "input1 = 'gun '\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "for s, proba in result:\n",
        "    print(f\"{bpe.decode(input1)}{bpe.decode(s[2:])};\\tlog proba: {proba}.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gun and with a smill;\tlog proba: 0.24096440857242407.\n",
            "gun been said not ;\tlog proba: 0.16245309491378124.\n",
            "gun and said not been;\tlog proba: 0.10901821889589187.\n",
            "gun and so mussion of ;\tlog proba: 0.03893031847254916.\n",
            "gun but was not been;\tlog proba: 0.03675416320132029.\n",
            "gun and so mussion hi;\tlog proba: 0.03249013770009046.\n",
            "gun and so must been ;\tlog proba: 0.03156503196883071.\n",
            "gun said not been ;\tlog proba: 0.025207662054596385.\n",
            "gun and said not see;\tlog proba: 0.021695852287233024.\n",
            "gun but was sold no;\tlog proba: 0.02008459268146401.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGGGnYWQzc4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc0f5132-a4b7-427a-9409-7e460ceb4bcc"
      },
      "source": [
        "def perplexity(snt, lm):\n",
        "    \"\"\"\n",
        "    snt: sequence of token ids\n",
        "    lm: language model\n",
        "    \"\"\"\n",
        "    result = 2 ** (-(sum([np.log(lm.get_proba(snt[i-2], snt[i-1], snt[i]))\n",
        "                          for i in range(2, len(snt))])) / (len(snt) - 2))\n",
        "    return result\n",
        "\n",
        "\n",
        "perplexity(tokenized_text[0], lm)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.092798992818707"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxxGAHTAK8mA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}